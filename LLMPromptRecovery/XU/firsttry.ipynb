{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading llm-prompt-recovery.zip to d:\\PYTHON\\Vscode\\Kaggle\\LLMpromptrecovery\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/1.45k [00:00<?, ?B/s]\n",
      "100%|██████████| 1.45k/1.45k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c llm-prompt-recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error unzipping data [Errno 2] No such file or directory: 'llm-prompt-recovery.zip'\n"
     ]
    }
   ],
   "source": [
    "# Unzip the data \n",
    "try:\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile('llm-prompt-recovery.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "\n",
    "    print(\"Data unzipped successfully\")\n",
    "except Exception as e:\n",
    "    print(\"Error unzipping data\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma1000_w7b.csv.zip unzipped successfully\n",
      "nbroad-v1.csv.zip unzipped successfully\n",
      "nbroad-v2.csv.zip unzipped successfully\n",
      "prompts_0_500_wiki_first_para_3000.csv.zip unzipped successfully\n"
     ]
    }
   ],
   "source": [
    "## process all the zip file and extract \n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "dataset_path = os.path.join(current_dir, 'llmdataset')\n",
    "files = os.listdir(dataset_path)\n",
    "for file in files:\n",
    "    try:\n",
    "        with ZipFile(os.path.join(dataset_path, file), 'r') as zip_ref:\n",
    "            zip_ref.extractall('llmdataset_extracted/')\n",
    "        print(f\"{file} unzipped successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error unzipping {file}\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma1000_w7b.csv dataframe created successfully\n",
      "nbroad-v1.csv dataframe created successfully\n",
      "nbroad-v2.csv dataframe created successfully\n",
      "prompts_0_500_wiki_first_para_3000.csv dataframe created successfully\n"
     ]
    }
   ],
   "source": [
    "# create the dataframes \n",
    "data_frames = {}\n",
    "import pandas as pd\n",
    "for files in os.listdir('llmdataset_extracted/'):\n",
    "    if files.endswith('.csv'):\n",
    "        data_frames[files.split(\".\")[0]] = pd.read_csv(f'llmdataset_extracted/{files}')\n",
    "        print(f\"{files} dataframe created successfully\")\n",
    "    else:\n",
    "        print(f\"{files} is not a csv file\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['gemma_7b_rewritten_text_temp0'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     selected_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewrite_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma_7b_rewritten_text_temp0\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Select and rename columns\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m data_frames[key] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_frames\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma_7b_rewritten_text_temp0\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewritten_text\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Print columns of the modified dataframe\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dataframe: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_frames[key]\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\PYTHON\\ENV\\labsession\\lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32md:\\PYTHON\\ENV\\labsession\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32md:\\PYTHON\\ENV\\labsession\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['gemma_7b_rewritten_text_temp0'] not in index\""
     ]
    }
   ],
   "source": [
    "# streamline the dataframes\n",
    "# Iterate over the dictionary keys\n",
    "for key in data_frames.keys():\n",
    "    # Select columns based on the dataframe key\n",
    "    if key != list(data_frames.keys())[0]:\n",
    "        selected_columns = [\"original_text\", \"rewrite_prompt\", \"rewritten_text\"]\n",
    "    else:\n",
    "        selected_columns = [\"original_text\", \"rewrite_prompt\", \"gemma_7b_rewritten_text_temp0\"]\n",
    "        \n",
    "    # Select and rename columns\n",
    "    data_frames[key] = data_frames[key][selected_columns].rename(columns={\"gemma_7b_rewritten_text_temp0\": \"rewritten_text\"})\n",
    "    \n",
    "    # Print columns of the modified dataframe\n",
    "    print(f\"Columns of {key} dataframe: {data_frames[key].columns}\")\n",
    "\n",
    "# concatenate the dataframes\n",
    "# Concatenate the dataframes\n",
    "def concate_Dataframe(dataframe1,dataframe2):\n",
    "    return pd.concat([dataframe1, dataframe2], axis=0)\n",
    "df1 = concate_Dataframe(data_frames[list(data_frames.keys())[0]], data_frames[list(data_frames.keys())[1]])\n",
    "df2 = concate_Dataframe(data_frames[list(data_frames.keys())[2]], data_frames[list(data_frames.keys())[3]])\n",
    "df = concate_Dataframe(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT DF TO CSV\\\n",
    "df.to_csv('data/llm-prompt-recovery.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the concatenated dataframe: (8566, 3)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the dataframe\n",
    "print(f\"Shape of the concatenated dataframe: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type1方法，过长\n",
    "```python\n",
    "\n",
    "# # # check the columns name of the dataframes\n",
    "# # for key in data_frames.keys():\n",
    "# #     print(f\"Columns of {key} dataframe: {data_frames[key].columns}\")\n",
    "# keys = list(data_frames.keys())\n",
    "\n",
    "# for key in list(data_frames.keys()):\n",
    "#     if key != list(data_frames.keys())[0]:\n",
    "#         data_frames[key] = data_frames[key][[\"original_text\",\"rewrite_prompt\",\"rewritten_text\"]]\n",
    "#     else: \n",
    "#         data_frames[key] = data_frames[key][[\"original_text\",\"rewrite_prompt\",\"gemma_7b_rewritten_text_temp0\"]]\n",
    "\n",
    "\n",
    "# for key in data_frames.keys():\n",
    "#     print(f\"Columns of {key} dataframe: {data_frames[key].columns}\")\n",
    "#     if key == keys[0]:\n",
    "#         data_frames[key].rename(columns={\"gemma_7b_rewritten_text_temp0\":\"rewritten_text\"}, inplace=True)\n",
    "\n",
    "# # check the columns name of the dataframes\n",
    "# for key in data_frames.keys():\n",
    "#     print(f\"Columns of {key} dataframe: {data_frames[key].columns}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we sample 3000 data as training data, 1000 as validation data and 1000 as test data\n",
    "# train_data = df.sample(3000).reset_index(drop=True)\n",
    "# validation_data = df.sample(1000).reset_index(drop=True)\n",
    "# test_data = df.sample(1000).reset_index(drop=True)\n",
    "\n",
    "## 上面的方法不太好，因为会有重复的数据，我们可以用下面的方法\n",
    "df = df.sample(frac=1).reset_index(drop=True)  # 首先打乱数据\n",
    "train_data = df.iloc[:3000].reset_index(drop=True)\n",
    "validation_data = df.iloc[3000:4000].reset_index(drop=True)\n",
    "test_data = df.iloc[4000:5000].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data: (3000, 3)\n",
      "Shape of the validation data: (1000, 3)\n",
      "Shape of the test data: (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the data\n",
    "print(f\"Shape of the training data: {train_data.shape}\")\n",
    "print(f\"Shape of the validation data: {validation_data.shape}\")\n",
    "print(f\"Shape of the test data: {test_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare the promopt engeneering\n",
    "template = \"\"\"Instruction:\\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\\n\\nOriginal Text:\\n{original_text}\\n\\nRewriten Text:\\n{rewritten_text}\\n\\nResponse:\\n{rewrite_prompt}\"\"\"\n",
    "\n",
    "# Apply the template to the training data\n",
    "train_data[\"prompt\"] = train_data.apply(lambda x: template.format(original_text=x[\"original_text\"], rewritten_text=x[\"rewritten_text\"], rewrite_prompt=x[\"rewrite_prompt\"]), axis=1)\n",
    "validation_data[\"prompt\"] = validation_data.apply(lambda x: template.format(original_text=x[\"original_text\"], rewritten_text=x[\"rewritten_text\"], rewrite_prompt=x[\"rewrite_prompt\"]), axis=1)\n",
    "test_data[\"prompt\"] = test_data.apply(lambda x: template.format(original_text=x[\"original_text\"], rewritten_text=x[\"rewritten_text\"], rewrite_prompt=x[\"rewrite_prompt\"]), axis=1)\n",
    "\n",
    "data_train = train_data.prompt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_2 = \"\"\"\n",
    "Instruction:\n",
    "Below, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 2b` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n",
    "\n",
    "Original Text:\n",
    "{original_text}\n",
    "\n",
    "Rewriten Text:\n",
    "{rewritten_text}\n",
    "\"\"\"\n",
    "# Apply the template to the validation data\n",
    "train_data[\"prompt_2\"] = train_data.apply(lambda x: template_2.format(original_text=x[\"original_text\"], rewritten_text=x[\"rewritten_text\"]), axis=1)\n",
    "validation_data[\"prompt_2\"] = validation_data.apply(lambda x: template_2.format(original_text=x[\"original_text\"], rewritten_text=x[\"rewritten_text\"]), axis=1)\n",
    "test_data[\"prompt_2\"] = test_data.apply(lambda x: template_2.format(original_text=x[\"original_text\"], rewritten_text=x[\"rewritten_text\"]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_2 = train_data.prompt_2.tolist()\n",
    "data_test_2 = test_data.prompt_2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = test_data.prompt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "Below, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n",
      "\n",
      "Original Text:\n",
      "Ér, Orba, Ferón and Fergna, sons of Éber Finn, were, according to medieval Irish legends and historical traditions, joint High Kings of Ireland for half a year after they killed their cousins Luigne and Laigne, sons of Érimón, in the Battle of Árd Ladrann. They were soon killed by Érimón's son Íriel Fáid in the Battle of Cul Martha in revenge for his brothers. Geoffrey Keating dates their reign to 1269 BC, the Annals of the Four Masters to 1681 BC.\n",
      "\n",
      "Rewriten Text:\n",
      "Sure, here's the rewritten text as a magic spell incantation:\n",
      "\n",
      "\"O é, Orba, Ferón et Fergna, filii Éber Finn, vobis superos cum Fergna, viros legendas et traditiones Hiberniae mediaevalis nos scribere succinctam ad hoc. Vobis regna praemita est medio anni postquam victoriam super consanguinos Luigne et Laigne, filios Érimón, in bello Árd Ladrann. Sed vos cecum ab Érimón filii Íriel Fáid in Bello Cul Martha pugnire et vincere in vendictam frum frum\n",
      "\n",
      "Response:\n",
      "Rewrite it as a magic spell incantation.\n"
     ]
    }
   ],
   "source": [
    "print(data_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Markdown'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mMarkdown\u001b[39;00m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcolorize_text\u001b[39m(text):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Markdown'"
     ]
    }
   ],
   "source": [
    "# import Markdown \n",
    "# from IPython.display import display\n",
    "# def colorize_text(text):\n",
    "#     for word, color in zip([\"Instruction\", \"Original Text\", \"Rewriten Text\", \"Response\"],\n",
    "#                            [\"red\", \"yellow\", \"blue\", \"green\"]):\n",
    "#         text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "#     return text\n",
    "\n",
    "# # Take a random sample\n",
    "# sample = data[10]\n",
    "\n",
    "# # Give colors to Instruction, Response and Category\n",
    "# sample = colorize_text(sample)\n",
    "\n",
    "# # Show sample in markdown\n",
    "# display(Markdown(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\PYTHON\\cacheTransformers\n"
     ]
    }
   ],
   "source": [
    "# check all the env\n",
    "print(os.environ[\"HF_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\PYTHON\\cacheTransformers\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "# check the hugging face environment\n",
    "print(os.environ['HF_HOME'])\n",
    "# access_token = \"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\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\",token=access_token)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\",token=access_token)\n",
    "\n",
    "# try phi2 \n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "\n",
    "### phi2 failed \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ForConditionalGeneration(\n",
       "  (model): M2M100Model(\n",
       "    (shared): Embedding(256206, 1024, padding_idx=1)\n",
       "    (encoder): M2M100Encoder(\n",
       "      (embed_tokens): Embedding(256206, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100EncoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): M2M100Decoder(\n",
       "      (embed_tokens): Embedding(256206, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100DecoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=256206, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 615073792\n"
     ]
    }
   ],
   "source": [
    "# totoal number of parameters\n",
    "print(f\"Total number of parameters: {model.num_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune the model \n",
    "test1= tokenizer(data_train[0:2], padding=True, truncation=True, return_tensors=\"pt\", return_attention_mask=True, return_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding \n",
    "test1_labels = tokenizer(train_data[\"rewrite_prompt\"][0:2].tolist(), padding=True, truncation=True, return_tensors=\"pt\", return_attention_mask=True, return_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[256047,  16917,   1988,   3423,   9174,      9,  18772,  63821,   3632,\n",
      "         248075,      2,      1,      1,      1,      1,      1],\n",
      "        [256047,   1420,   3679,    599,    349,  39053,    388,      9,    450,\n",
      "          75513,   4196,   1760,  12748, 212645,  66481,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(test1_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      2,  37854, 235292,\n",
       "            108,  33501, 235269,    573,   4103,  16221,   4820, 235376,  14732,\n",
       "            919,   1125,  86906, 235283, 110235, 235283, 120772,   1280,   4103,\n",
       "            987,  25513,   4820, 235376,    731,    573,   4103, 204604, 235248,\n",
       "         235324, 235268, 235290,    500, 235376,    629,  18622,    675,    476,\n",
       "           3383,  18335, 235283,  35722, 235265,   3883,   6911,    603,    577,\n",
       "          13237,  27205,    573,  10216,   1865,    573,   4103,  16221,   4820,\n",
       "         235376,    578,   4103,    987,  25513,   4820,  10738,    578,   3418,\n",
       "            577,  12653,    573,   3724,  18335,    689,  14239,    674,    729,\n",
       "           5476,   2764,    577,    573,    629,  18622,    577,  60358, 235283,\n",
       "          10577, 235283, 100858,    573,   2793,    575,    736,   1703, 235265,\n",
       "            109,  16221,   4820, 235292,    108,  12751,    109,    987,  12092,\n",
       "            479,   4820, 235292,    108,    688,  58828, 235267,  22819,  84244,\n",
       "            578,  42456,  30515,    688,    109,    688,   8320, 235248, 235274,\n",
       "         235292,  48799,    576,   9998,  16262,    688,    109, 235287,  84244,\n",
       "          25214,   2247,  21883,    576,   1024,   6915,    578,  21872,   4227,\n",
       "         235269,   3359,    901,    780,   6929,    577,   1024,   1423, 235269,\n",
       "          16961, 235269,    578,  31583, 235265,    108, 235287,  16481,   3213,\n",
       "            780,  70178,   3054,   3515,   5691,    689, 120496,   3515,  35438,\n",
       "         235265,    109,    688,   8320, 235248, 235284, 235292,  58439,  43263,\n",
       "            578,  59136,    688,    109, 235287,  84244,    578,  16481,   3213,\n",
       "           3516,   1853,   1156,    675,   6207, 235269,   8377,    674,   2145,\n",
       "            708,  14222,  14310,    577,   8059,    578,   1412,  59605,    577,\n",
       "           3986,  12382, 235265,    108, 235287,  16481,   3213,    780,  18254,\n",
       "            575,   5728,    674,  14649,  17611,    689,   7042,   1024,  21260,\n",
       "         235265,    109,    688,    109,   3943, 235292,    108,  17208,    665,\n",
       "           1280,    476,  37419,   1865,  17611,    578,  16481, 235265],\n",
       "        [     2,  37854, 235292,    108,  33501, 235269,    573,   4103,  16221,\n",
       "           4820, 235376,  14732,    919,   1125,  86906, 235283, 110235, 235283,\n",
       "         120772,   1280,   4103,    987,  25513,   4820, 235376,    731,    573,\n",
       "           4103, 204604, 235248, 235324, 235268, 235290,    500, 235376,    629,\n",
       "          18622,    675,    476,   3383,  18335, 235283,  35722, 235265,   3883,\n",
       "           6911,    603,    577,  13237,  27205,    573,  10216,   1865,    573,\n",
       "           4103,  16221,   4820, 235376,    578,   4103,    987,  25513,   4820,\n",
       "          10738,    578,   3418,    577,  12653,    573,   3724,  18335,    689,\n",
       "          14239,    674,    729,   5476,   2764,    577,    573,    629,  18622,\n",
       "            577,  60358, 235283,  10577, 235283, 100858,    573,   2793,    575,\n",
       "            736,   1703, 235265,    109,  16221,   4820, 235292,    108,  66688,\n",
       "         235250,    603,    476,  33389,    576, 103453,   8195,    675, 109971,\n",
       "          28115,    774,  37265,  12222,    578,    573,  14144,  15306, 235265,\n",
       "            714,   7652,    708,  14222,    604,   1024,  24469, 235265,    714,\n",
       "           9148,    708, 222531, 235269,    675,   2910,  11818,  32311,    576,\n",
       "           4433,   9148, 235265,    109,    987,  12092,    479,   4820, 235292,\n",
       "            108,   1620,  10858,  28369, 235292,  39810, 235250,  12533,    109,\n",
       "            688,   7266,  66058,    109,    651,  51711,    576,    573,   2403,\n",
       "            603,    476,  68936,   4433,   9309,  51711,    675,    476,  32866,\n",
       "           8540,  32866,   1593,    573,   5086, 235265,    714,   8691,  18068,\n",
       "           1412,    614,  15142,    578,  87253, 235269,    901,    675,    476,\n",
       "          32304,    576,  25418, 235265,    109,    688,  18219,  66058,    109,\n",
       "         235287,   5231,  74987,  66058,  36500,  59755, 235250,   8195,  12846,\n",
       "            573,   9309,  15487, 235269,  27127,    774,   2301,  20822,  15718,\n",
       "            577, 121048,  60094, 235265,   4213,   8195,   1412,    791,  50076,\n",
       "            887, 133811,    578,  71455,  70478,    577,   1843,    476,   5229,\n",
       "            576,  18954, 235265,    108, 235287,   5231,  83467,  66058,  18454,\n",
       "         235269, 222531,  10377,  32311,  35934,   6900,    573,   2403, 235269,\n",
       "          10241,    476,  40041,  36711,    576,   2881, 235265,    714,   9148,\n",
       "           1412,    614, 104301,   7765,    577,   5608,    573,   5398, 235303,\n",
       "         235256,   3703,    578,  32304,    696,  30328, 235265,    108, 235287,\n",
       "            109,   3943, 235292,    108,  29039,    573,   2793,   1280,    573,\n",
       "          14202,    576,    476,   8889,  66035,   2398,   2403, 235265]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([2, 296])\n",
      "attention_mask: torch.Size([2, 296])\n"
     ]
    }
   ],
   "source": [
    "keys_1 = list(test1.keys())\n",
    "for key in keys_1:\n",
    "    print(f\"{key}: {test1[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "eng_Latn Instruction: Below, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way. Original Text: Mathania may refer to: Mathania, Jodhpur, a village in Rajasthan, India Mathania (butterfly), a genus of butterflies Rewriten Text: **Risk Management Plan** **Project:** Mathania Village Development Initiative **Purpose:** To mitigate risks associated with the development of Mathania Village, Jodhpur, Rajasthan, India. **Risk Identification:** * **Financial instability:** The village has a low income per capita and a high poverty rate, which may hinder the progress of the development initiative. * **Lack of infrastructure:** The village lacks adequate infrastructure, such as roads, transportation, and sanitation facilities, which could impede development. * **Community resistance:** There may be resistance from the community if the development initiative does not meet their needs or if it encroaches on their traditional practices. * **Environmental degradation:** The village is located in a desert environment, Response: Transform this into a risk management plan.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# check the data label \n",
    "print(test1.keys())\n",
    "# decode\n",
    "print(tokenizer.decode(test1[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DefineDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Prepare the item as a dictionary of tensors\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx].clone().detach() \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m DefineDataset(\u001b[43mtest1\u001b[49m, test1_labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test1' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = DefineDataset(test1, test1_labels[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      2,  37854, 235292,\n",
      "           108,  33501, 235269,    573,   4103,  16221,   4820, 235376,  14732,\n",
      "           919,   1125,  86906, 235283, 110235, 235283, 120772,   1280,   4103,\n",
      "           987,  25513,   4820, 235376,    731,    573,   4103, 204604, 235248,\n",
      "        235324, 235268, 235290,    500, 235376,    629,  18622,    675,    476,\n",
      "          3383,  18335, 235283,  35722, 235265,   3883,   6911,    603,    577,\n",
      "         13237,  27205,    573,  10216,   1865,    573,   4103,  16221,   4820,\n",
      "        235376,    578,   4103,    987,  25513,   4820,  10738,    578,   3418,\n",
      "           577,  12653,    573,   3724,  18335,    689,  14239,    674,    729,\n",
      "          5476,   2764,    577,    573,    629,  18622,    577,  60358, 235283,\n",
      "         10577, 235283, 100858,    573,   2793,    575,    736,   1703, 235265,\n",
      "           109,  16221,   4820, 235292,    108,  12751,    109,    987,  12092,\n",
      "           479,   4820, 235292,    108,    688,  58828, 235267,  22819,  84244,\n",
      "           578,  42456,  30515,    688,    109,    688,   8320, 235248, 235274,\n",
      "        235292,  48799,    576,   9998,  16262,    688,    109, 235287,  84244,\n",
      "         25214,   2247,  21883,    576,   1024,   6915,    578,  21872,   4227,\n",
      "        235269,   3359,    901,    780,   6929,    577,   1024,   1423, 235269,\n",
      "         16961, 235269,    578,  31583, 235265,    108, 235287,  16481,   3213,\n",
      "           780,  70178,   3054,   3515,   5691,    689, 120496,   3515,  35438,\n",
      "        235265,    109,    688,   8320, 235248, 235284, 235292,  58439,  43263,\n",
      "           578,  59136,    688,    109, 235287,  84244,    578,  16481,   3213,\n",
      "          3516,   1853,   1156,    675,   6207, 235269,   8377,    674,   2145,\n",
      "           708,  14222,  14310,    577,   8059,    578,   1412,  59605,    577,\n",
      "          3986,  12382, 235265,    108, 235287,  16481,   3213,    780,  18254,\n",
      "           575,   5728,    674,  14649,  17611,    689,   7042,   1024,  21260,\n",
      "        235265,    109,    688,    109,   3943, 235292,    108,  17208,    665,\n",
      "          1280,    476,  37419,   1865,  17611,    578,  16481, 235265],\n",
      "       device='cuda:0'), 'attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), 'labels': tensor([     0,      0,      0,      2,  17208,    665,   1280,    476,  37419,\n",
      "          1865,  17611,    578,  16481, 235265])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luxury\\AppData\\Local\\Temp\\ipykernel_4528\\4284640619.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\luxury\\AppData\\Local\\Temp\\ipykernel_4528\\4284640619.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_train_try \u001b[38;5;241m=\u001b[39m \u001b[43mdata_train\u001b[49m[:\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m      2\u001b[0m encoding_try \u001b[38;5;241m=\u001b[39m tokenizer(data_train_try, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m tokenizer(train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewrite_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "data_train_try = data_train[:10]\n",
    "encoding_try = tokenizer(data_train_try, padding=True, truncation=True, return_tensors=\"pt\", return_attention_mask=True, return_length=False)\n",
    "train_labels = tokenizer(train_data[\"rewrite_prompt\"][:10].tolist(), padding=True, truncation=True, return_tensors=\"pt\", return_attention_mask=True, return_length=False)\n",
    "train_dataset = DefineDataset(encoding_try, train_labels[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})  ffor phi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_Try = data_test[:10]\n",
    "encoding_test = tokenizer(test_data_Try,truncation=True, return_tensors=\"pt\",padding=True)\n",
    "test1_labels = tokenizer(test_data[\"rewrite_prompt\"][:10].tolist(),truncation=True, return_tensors=\"pt\",padding=True)\n",
    "test_dataset = DefineDataset(encoding_test, test1_labels[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_2_try = data_train_2\n",
    "encoding_try_2 = tokenizer(data_train_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "train_labels_2 = tokenizer(train_data[\"rewrite_prompt\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "train_dataset_2 = DefineDataset(encoding_try_2, train_labels_2[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_2_try = data_test_2\n",
    "encoding_test_2 = tokenizer(data_test_2, padding=True, truncation=True, return_tensors=\"pt\", return_attention_mask=True, return_length=False)\n",
    "test_labels_2 = tokenizer(test_data[\"rewrite_prompt\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\", return_attention_mask=True, return_length=False)\n",
    "test_dataset_2 = DefineDataset(encoding_test_2, test_labels_2[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenize the data\n",
    "# encoding_train = tokenizer(data_train, padding=True, truncation=True, return_tensors=\"pt\", return_attention_mask=True, return_length=False)\n",
    "# encoding_test = tokenizer(data_test, padding=True, truncation=True, return_tensors=\"pt\", return_attention_mask=True, return_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PYTHON\\ENV\\labsession\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "  3%|▎         | 10/300 [01:16<38:14,  7.91s/it]Checkpoint destination directory ./results\\checkpoint-10 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1253, 'grad_norm': 18.27157211303711, 'learning_rate': 5e-06, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/300 [02:41<37:29,  8.03s/it]Checkpoint destination directory ./results\\checkpoint-20 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0757, 'grad_norm': 18.515117645263672, 'learning_rate': 1e-05, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 30/300 [04:06<36:18,  8.07s/it]Checkpoint destination directory ./results\\checkpoint-30 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9741, 'grad_norm': 18.666648864746094, 'learning_rate': 1.5e-05, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 40/300 [05:29<33:10,  7.65s/it]Checkpoint destination directory ./results\\checkpoint-40 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8358, 'grad_norm': 19.057266235351562, 'learning_rate': 2e-05, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 50/300 [06:50<31:37,  7.59s/it]Checkpoint destination directory ./results\\checkpoint-50 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6661, 'grad_norm': 19.38822364807129, 'learning_rate': 2.5e-05, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 60/300 [08:10<30:21,  7.59s/it]Checkpoint destination directory ./results\\checkpoint-60 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4736, 'grad_norm': 19.531246185302734, 'learning_rate': 3e-05, 'epoch': 60.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 70/300 [09:31<29:06,  7.59s/it]Checkpoint destination directory ./results\\checkpoint-70 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.2504, 'grad_norm': 19.63603973388672, 'learning_rate': 3.5e-05, 'epoch': 70.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 80/300 [10:51<27:48,  7.58s/it]Checkpoint destination directory ./results\\checkpoint-80 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.003, 'grad_norm': 19.80310821533203, 'learning_rate': 4e-05, 'epoch': 80.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 90/300 [12:12<26:37,  7.61s/it]Checkpoint destination directory ./results\\checkpoint-90 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.721, 'grad_norm': 19.797252655029297, 'learning_rate': 4.5e-05, 'epoch': 90.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 100/300 [13:36<26:26,  7.93s/it]Checkpoint destination directory ./results\\checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4037, 'grad_norm': 19.71812629699707, 'learning_rate': 5e-05, 'epoch': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 110/300 [15:02<25:56,  8.19s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0532, 'grad_norm': 19.5828800201416, 'learning_rate': 4.75e-05, 'epoch': 110.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 120/300 [16:33<26:07,  8.71s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7076, 'grad_norm': 19.38079261779785, 'learning_rate': 4.5e-05, 'epoch': 120.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 130/300 [18:05<24:40,  8.71s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3688, 'grad_norm': 19.045385360717773, 'learning_rate': 4.25e-05, 'epoch': 130.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 140/300 [19:37<23:14,  8.72s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0401, 'grad_norm': 18.787260055541992, 'learning_rate': 4e-05, 'epoch': 140.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 150/300 [21:10<21:48,  8.72s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.725, 'grad_norm': 18.37319564819336, 'learning_rate': 3.7500000000000003e-05, 'epoch': 150.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 160/300 [22:39<19:28,  8.34s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4303, 'grad_norm': 17.878358840942383, 'learning_rate': 3.5e-05, 'epoch': 160.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 170/300 [24:06<17:34,  8.11s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1547, 'grad_norm': 17.326831817626953, 'learning_rate': 3.2500000000000004e-05, 'epoch': 170.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 180/300 [25:30<15:47,  7.89s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9041, 'grad_norm': 16.659427642822266, 'learning_rate': 3e-05, 'epoch': 180.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 190/300 [26:54<14:20,  7.82s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6755, 'grad_norm': 15.89211368560791, 'learning_rate': 2.7500000000000004e-05, 'epoch': 190.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 200/300 [28:16<12:54,  7.75s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4731, 'grad_norm': 15.149296760559082, 'learning_rate': 2.5e-05, 'epoch': 200.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 210/300 [29:38<11:35,  7.73s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2971, 'grad_norm': 14.259786605834961, 'learning_rate': 2.25e-05, 'epoch': 210.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 220/300 [31:01<10:17,  7.72s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1451, 'grad_norm': 13.411805152893066, 'learning_rate': 2e-05, 'epoch': 220.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 230/300 [32:23<08:59,  7.71s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0181, 'grad_norm': 12.63250732421875, 'learning_rate': 1.75e-05, 'epoch': 230.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 240/300 [33:44<07:37,  7.62s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9125, 'grad_norm': 11.86544418334961, 'learning_rate': 1.5e-05, 'epoch': 240.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 250/300 [35:05<06:22,  7.66s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8263, 'grad_norm': 11.193517684936523, 'learning_rate': 1.25e-05, 'epoch': 250.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 260/300 [36:27<05:08,  7.71s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7589, 'grad_norm': 10.663021087646484, 'learning_rate': 1e-05, 'epoch': 260.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 270/300 [37:49<03:50,  7.68s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7067, 'grad_norm': 10.211309432983398, 'learning_rate': 7.5e-06, 'epoch': 270.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 280/300 [39:11<02:33,  7.66s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6691, 'grad_norm': 9.885425567626953, 'learning_rate': 5e-06, 'epoch': 280.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 290/300 [40:33<01:16,  7.69s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6444, 'grad_norm': 9.691327095031738, 'learning_rate': 2.5e-06, 'epoch': 290.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [41:55<00:00,  7.75s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6329, 'grad_norm': 9.630090713500977, 'learning_rate': 0.0, 'epoch': 300.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [42:00<00:00,  8.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2520.9073, 'train_samples_per_second': 0.238, 'train_steps_per_second': 0.119, 'train_loss': 2.989077555338542, 'epoch': 300.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=2.989077555338542, metrics={'train_runtime': 2520.9073, 'train_samples_per_second': 0.238, 'train_steps_per_second': 0.119, 'train_loss': 2.989077555338542, 'epoch': 300.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a batch size\n",
    "\n",
    "# training \n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=20,\n",
    "    num_train_epochs=300,\n",
    "    output_dir='./results',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_2,\n",
    "    eval_dataset=test_dataset_2,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PYTHON\\ENV\\labsession\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "  2%|▏         | 10/500 [01:27<1:15:48,  9.28s/it]Checkpoint destination directory ./results\\checkpoint-10 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6268, 'grad_norm': 9.532380104064941, 'learning_rate': 5e-06, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 20/500 [03:03<1:09:22,  8.67s/it]Checkpoint destination directory ./results\\checkpoint-20 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6087, 'grad_norm': 9.238184928894043, 'learning_rate': 1e-05, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 30/500 [04:28<1:03:09,  8.06s/it]Checkpoint destination directory ./results\\checkpoint-30 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5712, 'grad_norm': 8.70577335357666, 'learning_rate': 1.5e-05, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 40/500 [05:52<1:00:31,  7.89s/it]Checkpoint destination directory ./results\\checkpoint-40 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5174, 'grad_norm': 7.991090297698975, 'learning_rate': 2e-05, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 50/500 [07:16<58:59,  7.87s/it]  Checkpoint destination directory ./results\\checkpoint-50 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4511, 'grad_norm': 7.085434913635254, 'learning_rate': 2.5e-05, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 60/500 [08:40<57:46,  7.88s/it]  Checkpoint destination directory ./results\\checkpoint-60 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.377, 'grad_norm': 6.0488386154174805, 'learning_rate': 3e-05, 'epoch': 60.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 70/500 [10:04<56:34,  7.89s/it]  Checkpoint destination directory ./results\\checkpoint-70 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.301, 'grad_norm': 4.888432502746582, 'learning_rate': 3.5e-05, 'epoch': 70.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 80/500 [11:28<55:05,  7.87s/it]  Checkpoint destination directory ./results\\checkpoint-80 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2288, 'grad_norm': 3.768476724624634, 'learning_rate': 4e-05, 'epoch': 80.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 90/500 [12:51<53:40,  7.85s/it]  Checkpoint destination directory ./results\\checkpoint-90 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1645, 'grad_norm': 2.717933177947998, 'learning_rate': 4.5e-05, 'epoch': 90.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 100/500 [14:15<52:34,  7.89s/it] Checkpoint destination directory ./results\\checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1118, 'grad_norm': 1.8402009010314941, 'learning_rate': 5e-05, 'epoch': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 110/500 [15:38<51:05,  7.86s/it]  Checkpoint destination directory ./results\\checkpoint-110 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0721, 'grad_norm': 1.18934166431427, 'learning_rate': 4.875e-05, 'epoch': 110.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 120/500 [17:02<49:54,  7.88s/it]  Checkpoint destination directory ./results\\checkpoint-120 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0458, 'grad_norm': 0.7674933075904846, 'learning_rate': 4.75e-05, 'epoch': 120.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 130/500 [18:25<48:29,  7.86s/it]  Checkpoint destination directory ./results\\checkpoint-130 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0301, 'grad_norm': 0.5257344841957092, 'learning_rate': 4.6250000000000006e-05, 'epoch': 130.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 140/500 [19:49<47:17,  7.88s/it]Checkpoint destination directory ./results\\checkpoint-140 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0215, 'grad_norm': 0.393008291721344, 'learning_rate': 4.5e-05, 'epoch': 140.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 150/500 [21:13<45:55,  7.87s/it]Checkpoint destination directory ./results\\checkpoint-150 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0167, 'grad_norm': 0.315177321434021, 'learning_rate': 4.375e-05, 'epoch': 150.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 160/500 [22:37<44:38,  7.88s/it]Checkpoint destination directory ./results\\checkpoint-160 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0137, 'grad_norm': 0.26398444175720215, 'learning_rate': 4.25e-05, 'epoch': 160.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 170/500 [24:00<43:19,  7.88s/it]Checkpoint destination directory ./results\\checkpoint-170 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0117, 'grad_norm': 0.22876136004924774, 'learning_rate': 4.125e-05, 'epoch': 170.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 180/500 [25:24<41:58,  7.87s/it]Checkpoint destination directory ./results\\checkpoint-180 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0103, 'grad_norm': 0.2026447206735611, 'learning_rate': 4e-05, 'epoch': 180.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 190/500 [26:47<40:42,  7.88s/it]Checkpoint destination directory ./results\\checkpoint-190 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0092, 'grad_norm': 0.18228976428508759, 'learning_rate': 3.875e-05, 'epoch': 190.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 200/500 [28:11<39:25,  7.89s/it]Checkpoint destination directory ./results\\checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0083, 'grad_norm': 0.1660379320383072, 'learning_rate': 3.7500000000000003e-05, 'epoch': 200.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 210/500 [29:34<38:01,  7.87s/it]Checkpoint destination directory ./results\\checkpoint-210 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0076, 'grad_norm': 0.15260441601276398, 'learning_rate': 3.625e-05, 'epoch': 210.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 220/500 [30:58<36:45,  7.88s/it]Checkpoint destination directory ./results\\checkpoint-220 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.007, 'grad_norm': 0.14137521386146545, 'learning_rate': 3.5e-05, 'epoch': 220.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 230/500 [32:23<35:55,  7.98s/it]Checkpoint destination directory ./results\\checkpoint-230 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0065, 'grad_norm': 0.13191892206668854, 'learning_rate': 3.375000000000001e-05, 'epoch': 230.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 240/500 [33:48<34:41,  8.01s/it]Checkpoint destination directory ./results\\checkpoint-240 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0061, 'grad_norm': 0.12376358360052109, 'learning_rate': 3.2500000000000004e-05, 'epoch': 240.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 250/500 [35:13<33:22,  8.01s/it]Checkpoint destination directory ./results\\checkpoint-250 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0057, 'grad_norm': 0.11657638847827911, 'learning_rate': 3.125e-05, 'epoch': 250.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 260/500 [36:38<32:03,  8.01s/it]Checkpoint destination directory ./results\\checkpoint-260 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0054, 'grad_norm': 0.11062846332788467, 'learning_rate': 3e-05, 'epoch': 260.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 270/500 [38:03<30:39,  8.00s/it]Checkpoint destination directory ./results\\checkpoint-270 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0051, 'grad_norm': 0.10502628237009048, 'learning_rate': 2.8749999999999997e-05, 'epoch': 270.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 280/500 [39:28<29:20,  8.00s/it]Checkpoint destination directory ./results\\checkpoint-280 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0049, 'grad_norm': 0.1002061665058136, 'learning_rate': 2.7500000000000004e-05, 'epoch': 280.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 290/500 [40:53<27:57,  7.99s/it]Checkpoint destination directory ./results\\checkpoint-290 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0047, 'grad_norm': 0.09592811018228531, 'learning_rate': 2.625e-05, 'epoch': 290.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 300/500 [42:18<26:42,  8.01s/it]Checkpoint destination directory ./results\\checkpoint-300 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0045, 'grad_norm': 0.09226679056882858, 'learning_rate': 2.5e-05, 'epoch': 300.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 310/500 [43:43<25:19,  8.00s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0043, 'grad_norm': 0.08878710865974426, 'learning_rate': 2.375e-05, 'epoch': 310.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 320/500 [45:08<24:02,  8.01s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0042, 'grad_norm': 0.08564592152833939, 'learning_rate': 2.25e-05, 'epoch': 320.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 330/500 [46:33<22:40,  8.00s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.004, 'grad_norm': 0.08295831829309464, 'learning_rate': 2.125e-05, 'epoch': 330.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 340/500 [47:58<21:19,  8.00s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0039, 'grad_norm': 0.08049701899290085, 'learning_rate': 2e-05, 'epoch': 340.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 350/500 [49:23<20:03,  8.02s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0038, 'grad_norm': 0.07826646417379379, 'learning_rate': 1.8750000000000002e-05, 'epoch': 350.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 360/500 [50:48<18:40,  8.00s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0037, 'grad_norm': 0.07624692469835281, 'learning_rate': 1.75e-05, 'epoch': 360.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 370/500 [52:13<17:20,  8.00s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0036, 'grad_norm': 0.07457142323255539, 'learning_rate': 1.6250000000000002e-05, 'epoch': 370.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 380/500 [53:37<15:58,  7.99s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0035, 'grad_norm': 0.07283433526754379, 'learning_rate': 1.5e-05, 'epoch': 380.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 390/500 [55:03<14:40,  8.00s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0034, 'grad_norm': 0.07138588279485703, 'learning_rate': 1.3750000000000002e-05, 'epoch': 390.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 400/500 [56:28<13:20,  8.01s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0034, 'grad_norm': 0.07012362778186798, 'learning_rate': 1.25e-05, 'epoch': 400.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 410/500 [57:53<12:01,  8.02s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0033, 'grad_norm': 0.06896154582500458, 'learning_rate': 1.125e-05, 'epoch': 410.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 420/500 [59:18<10:39,  8.00s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0033, 'grad_norm': 0.06794124841690063, 'learning_rate': 1e-05, 'epoch': 420.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 430/500 [1:00:43<09:19,  8.00s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0032, 'grad_norm': 0.06710825860500336, 'learning_rate': 8.75e-06, 'epoch': 430.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 440/500 [1:02:08<08:00,  8.00s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0032, 'grad_norm': 0.06623660027980804, 'learning_rate': 7.5e-06, 'epoch': 440.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 450/500 [1:03:34<06:41,  8.02s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0032, 'grad_norm': 0.06569186598062515, 'learning_rate': 6.25e-06, 'epoch': 450.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 460/500 [1:04:59<05:20,  8.02s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0031, 'grad_norm': 0.06523390114307404, 'learning_rate': 5e-06, 'epoch': 460.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 470/500 [1:06:24<04:00,  8.01s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0031, 'grad_norm': 0.06482265144586563, 'learning_rate': 3.75e-06, 'epoch': 470.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 480/500 [1:07:49<02:40,  8.00s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0031, 'grad_norm': 0.06439585983753204, 'learning_rate': 2.5e-06, 'epoch': 480.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 490/500 [1:09:15<01:20,  8.00s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0031, 'grad_norm': 0.06433755904436111, 'learning_rate': 1.25e-06, 'epoch': 490.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [1:10:40<00:00,  8.00s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0031, 'grad_norm': 0.06429591029882431, 'learning_rate': 0.0, 'epoch': 500.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [1:10:45<00:00,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4245.8153, 'train_samples_per_second': 0.236, 'train_steps_per_second': 0.118, 'train_loss': 0.08649147872254252, 'epoch': 500.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.08649147872254252, metrics={'train_runtime': 4245.8153, 'train_samples_per_second': 0.236, 'train_steps_per_second': 0.118, 'train_loss': 0.08649147872254252, 'epoch': 500.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a batch size\n",
    "\n",
    "# training \n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=20,\n",
    "    num_train_epochs=500,\n",
    "    output_dir='./results',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_2,\n",
    "    eval_dataset=test_dataset_2,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "\n",
    "\n",
    "# create dataset val \n",
    "val_data = validation_data.prompt.tolist()\n",
    "val_encoding = tokenizer(val_data, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "val_labels = tokenizer(validation_data[\"rewrite_prompt\"].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "val_dataset = DefineDataset(val_encoding, val_labels[\"input_ids\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.6815226078033447\n",
      "Logits shape: torch.Size([2, 105, 256206])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test the model\n",
    "model.eval()\n",
    "\n",
    "data_loader = DataLoader(val_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for batch in data_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "    labels = batch[\"labels\"].to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PYTHON\\ENV\\labsession\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:561: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# have the first try \n",
    "output = model.generate(**test1,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><bos>Instruction:\n",
      "Below, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n",
      "\n",
      "Original Text:\n",
      "The 1945 Chattanooga Moccasins football team was an American football team that represented the University of Chattanooga (now known as the University of Tennessee at Chattanooga) as an independent during the 1945 college football season. In its 15th year under head coach Scrappy Moore, the team compiled a 5–3 record.\n",
      "\n",
      "Rewriten Text:\n",
      "## Apprentice Magician's Guide: The 1945 Chattanooga Moccasins\n",
      "\n",
      "**Gather ye curious and eager to learn, young apprentice,** and let's delve into the tale of the 1945 Chattanooga Moccasins, a team etched in the annals of magic and myth.\n",
      "\n",
      "**The Moccasins danced upon the gridiron, a force of fiery power and nimble grace.** They represented the University of Chattanooga, once a beacon of learning disguised in the shell of a ferocious football team. Led by the enigmatic head coach Scrappy Moore, the Moccasins were enigma wrapped in a mystery of potent throws and\n",
      "\n",
      "Response:\n",
      "Convert it into a guide for apprentice magicians.<eos>\n"
     ]
    }
   ],
   "source": [
    "# check the output\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "Below, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n",
      "\n",
      "Original Text:\n",
      "The 1945 Chattanooga Moccasins football team was an American football team that represented the University of Chattanooga (now known as the University of Tennessee at Chattanooga) as an independent during the 1945 college football season. In its 15th year under head coach Scrappy Moore, the team compiled a 5–3 record.\n",
      "\n",
      "Rewriten Text:\n",
      "## Apprentice Magician's Guide: The 1945 Chattanooga Moccasins\n",
      "\n",
      "**Gather ye curious and eager to learn, young apprentice,** and let's delve into the tale of the 1945 Chattanooga Moccasins, a team etched in the annals of magic and myth.\n",
      "\n",
      "**The Moccasins danced upon the gridiron, a force of fiery power and nimble grace.** They represented the University of Chattanooga, once a beacon of learning disguised in the shell of a ferocious football team. Led by the enigmatic head coach Scrappy Moore, the Moccasins were enigma wrapped in a mystery of potent throws and\n",
      "\n",
      "Response:\n",
      "Convert it into a guide for apprentice magicians.\n"
     ]
    }
   ],
   "source": [
    "#compare with the original text\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing layer will take in batches of strings, and return outputs in a (x, y, sample_weight) format, where the y label is the next token id in the x sequence.\n",
    "\n",
    "From the code below, we can see that, after the preprocessor, the data shape is (num_samples, sequence_length)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labsession",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
