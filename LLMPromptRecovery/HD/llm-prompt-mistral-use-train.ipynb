{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":3153985,"sourceType":"datasetVersion","datasetId":1919065},{"sourceId":7747717,"sourceType":"datasetVersion","datasetId":4506214},{"sourceId":7893017,"sourceType":"datasetVersion","datasetId":4634330},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":386,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":284},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900}],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/hotchpotch/llm-detect-pip \n!pip install -q -U accelerate --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:00:31.929531Z","iopub.execute_input":"2024-04-13T23:00:31.930381Z","iopub.status.idle":"2024-04-13T23:01:17.353988Z","shell.execute_reply.started":"2024-04-13T23:00:31.930344Z","shell.execute_reply":"2024-04-13T23:01:17.352743Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:01:17.357155Z","iopub.execute_input":"2024-04-13T23:01:17.358066Z","iopub.status.idle":"2024-04-13T23:01:21.244420Z","shell.execute_reply.started":"2024-04-13T23:01:17.358025Z","shell.execute_reply":"2024-04-13T23:01:21.243612Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# define the runtime mode\nfrom enum import Enum\nclass Mode(Enum):\n    SUBMISSION = 0\n    DEBUG = 1\nMODE = Mode.SUBMISSION","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:09:07.855041Z","iopub.execute_input":"2024-04-13T23:09:07.855451Z","iopub.status.idle":"2024-04-13T23:09:07.860774Z","shell.execute_reply.started":"2024-04-13T23:09:07.855421Z","shell.execute_reply":"2024-04-13T23:09:07.859798Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"#  Data Preparation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nmatch MODE:\n    case Mode.DEBUG:   \n        train = pd.read_csv(\"/kaggle/input/gemma-rewrite-nbroad/nbroad-v1.csv\")\n        gross_test = pd.read_csv(\"/kaggle/input/gemma-rewrite-nbroad/nbroad-v2.csv\")\n        indexes = np.random.randint(0, gross_test.shape[1], 20)\n        gross_test = gross_test.iloc[indexes]\n        test = gross_test[['id', 'original_text', 'rewritten_text']]\n        target = gross_test[['id', 'rewrite_prompt']]\n    case Mode.SUBMISSION:\n        train = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/train.csv\")\n        test = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n    case _:\n        print(\"Mode error\")\n        raise Exception(\"Mode Error : Please choose a mode\")","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:09:08.364928Z","iopub.execute_input":"2024-04-13T23:09:08.365298Z","iopub.status.idle":"2024-04-13T23:09:08.378001Z","shell.execute_reply.started":"2024-04-13T23:09:08.365254Z","shell.execute_reply":"2024-04-13T23:09:08.376771Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display\n\n# preview\nif MODE==Mode.DEBUG:\n    display(test.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:09:08.673845Z","iopub.execute_input":"2024-04-13T23:09:08.674275Z","iopub.status.idle":"2024-04-13T23:09:08.679468Z","shell.execute_reply.started":"2024-04-13T23:09:08.674237Z","shell.execute_reply":"2024-04-13T23:09:08.678310Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"# Build instruction","metadata":{}},{"cell_type":"code","source":"# # Template - chat format version 1\n# def template(Ot, Rt, Rp=None):\n#     return [\n#         {\"role\": \"user\", \"content\": f\"Original text:\\n{Ot}\"},\n#         {\"role\": \"assistant\", \"content\": \"Provide the rewritten test\"}, # new text and I will tell you the prompt that can generate the original text to the new text.\"},\n#         {\"role\": \"user\", \"content\": f\"Rewritten text:\\n{Rt}\"},\n#         {\"role\": \"assistant\", \"content\": f\"Give the instruction\"},\n#         {\"role\": \"user\", \"content\": f\"Give the prompt that can generate the original text to the rewritten text\"},\n#     ] + ([{\"role\": \"assistant\", \"content\": Rp}] if Rp else [])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Template - chat format version 2\ndef template(Ot, Rt, Rp=None):\n    return [\n        {\n            \"role\": \"user\", \n            \"content\": f\"<original_text>\\n{Ot}\\n</original_text>\\n\"\\\n                        f\"<rewritten_text>\\n{Rt}\\n</rewritten_text>\\n\" \\\n                        f\"Write a prompt that was likely given to the LLM to rewrite original_text into rewritten_text.\"\n        }\n    ] + ([{\"role\": \"assistant\", \"content\": Rp}] if Rp else [])","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:10:33.642792Z","iopub.execute_input":"2024-04-13T23:10:33.643686Z","iopub.status.idle":"2024-04-13T23:10:33.649557Z","shell.execute_reply.started":"2024-04-13T23:10:33.643650Z","shell.execute_reply":"2024-04-13T23:10:33.648571Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef make_instruction(sentence, num_example=5, train_df=train):\n    # choose 5 example randomly from train\n    indexes = np.random.randint(0, train_df.shape[0], num_example)\n    instruction = []\n    for item in train_df.iloc[indexes].iloc:\n        instruction += template(item['original_text'], item['rewritten_text'], item['rewrite_prompt'])\n    # then give the test instruction\n    return instruction + template(sentence['original_text'], sentence['rewritten_text'])","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:10:35.790683Z","iopub.execute_input":"2024-04-13T23:10:35.791520Z","iopub.status.idle":"2024-04-13T23:10:35.797923Z","shell.execute_reply.started":"2024-04-13T23:10:35.791476Z","shell.execute_reply":"2024-04-13T23:10:35.796961Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# test\nif MODE == Mode.DEBUG:\n    display(make_instruction(test.iloc[0]))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:10:36.128749Z","iopub.execute_input":"2024-04-13T23:10:36.129136Z","iopub.status.idle":"2024-04-13T23:10:36.134217Z","shell.execute_reply.started":"2024-04-13T23:10:36.129107Z","shell.execute_reply":"2024-04-13T23:10:36.133146Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"# model_name  = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\nmodel_name  = \"/kaggle/input/mistral-7b-it-v02\"","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:01:21.602023Z","iopub.execute_input":"2024-04-13T23:01:21.602329Z","iopub.status.idle":"2024-04-13T23:01:21.608149Z","shell.execute_reply.started":"2024-04-13T23:01:21.602292Z","shell.execute_reply":"2024-04-13T23:01:21.606998Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    quantization_config=quantization_config,\n    low_cpu_mem_usage=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:01:21.609500Z","iopub.execute_input":"2024-04-13T23:01:21.609842Z","iopub.status.idle":"2024-04-13T23:03:02.298308Z","shell.execute_reply.started":"2024-04-13T23:01:21.609817Z","shell.execute_reply":"2024-04-13T23:03:02.297220Z"},"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dab1b35801dd464f87902c585a53a1f9"}},"metadata":{}},{"name":"stderr","text":"You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n","output_type":"stream"}]},{"cell_type":"code","source":"# load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:03:02.299959Z","iopub.execute_input":"2024-04-13T23:03:02.300606Z","iopub.status.idle":"2024-04-13T23:03:02.424394Z","shell.execute_reply.started":"2024-04-13T23:03:02.300567Z","shell.execute_reply":"2024-04-13T23:03:02.423530Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"# import torch\n# def get_inputs_batch(sub_df, tokenizer=tokenizer):\n#     tokenizer.pad_token = tokenizer.eos_token\n#     return [tokenizer.apply_chat_template(make_instruction(line), return_tensors=\"pt\") for line in sub_df.iloc]","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:03:02.425646Z","iopub.execute_input":"2024-04-13T23:03:02.426009Z","iopub.status.idle":"2024-04-13T23:03:02.430968Z","shell.execute_reply.started":"2024-04-13T23:03:02.425976Z","shell.execute_reply":"2024-04-13T23:03:02.429863Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# predict batch\n# def predict(df, model=model, tokenizer=tokenizer, batch_size=None, device=device):\n# #     if batch_size:\n# #         index_loader = DataLoader([i for i in range(df.size)], batch_size=batch_size)\n# #         for indexes in tqdm(index_loader):\n# #             batch_df = df.iloc[indexes]\n#     prompts = []\n#     for line in tqdm(df.iloc):   \n#         messages = make_instruction(line)\n#         tokenizer.pad_token = tokenizer.eos_token\n#         inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n#         inputs = inputs.to(device)\n#         with torch.no_grad():\n#             outputs = model.generate(inputs, max_new_tokens=500, pad_token_id=tokenizer.eos_token_id).cpu()\n#         answer = tokenizer.batch_decode(outputs)[0]\n#         try:\n#             result = answer.split(\"[/INST]\")[-1].replace(\"</s>\",\"\").strip()\n#         except:\n#             print(\"<START>\\nThis context is SB.\\n\"+answer+\"\\n<END>\")\n# #             raise Exception(\"Sorry, this sentence error\")\n#             result = answer\n#         prompts.append(result)\n#     return prompts\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:03:02.434975Z","iopub.execute_input":"2024-04-13T23:03:02.435291Z","iopub.status.idle":"2024-04-13T23:03:02.444082Z","shell.execute_reply.started":"2024-04-13T23:03:02.435251Z","shell.execute_reply":"2024-04-13T23:03:02.443181Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def predict(line, model=model, tokenizer=tokenizer, device=device, num_example=5): \n    messages = make_instruction(line, num_example=num_example)\n    tokenizer.pad_token = tokenizer.eos_token\n    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n    if MODE == Mode.DEBUG:\n        print(inputs.shape)\n    inputs = inputs.to(device)\n    with torch.no_grad():\n        outputs = model.generate(inputs, max_new_tokens=30, pad_token_id=tokenizer.eos_token_id).cpu()\n    answer = tokenizer.batch_decode(outputs)[0]\n    try:\n        return answer.split(\"[/INST]\")[-1].replace(\"</s>\",\"\").strip()\n    except:\n        print(\"<START>\\nThis context is SB.\\n\"+answer+\"\\n<END>\")\n#             raise Exception(\"Sorry, this sentence error\")\n        return answer","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:11:28.834813Z","iopub.execute_input":"2024-04-13T23:11:28.835498Z","iopub.status.idle":"2024-04-13T23:11:28.843547Z","shell.execute_reply.started":"2024-04-13T23:11:28.835460Z","shell.execute_reply":"2024-04-13T23:11:28.842484Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# test one\nif MODE==Mode.DEBUG:\n    prompts = predict(test.iloc[12])\n    print(prompts)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:11:32.857557Z","iopub.execute_input":"2024-04-13T23:11:32.857974Z","iopub.status.idle":"2024-04-13T23:11:32.863080Z","shell.execute_reply.started":"2024-04-13T23:11:32.857943Z","shell.execute_reply":"2024-04-13T23:11:32.862115Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# apply predict function to each line\nfrom tqdm import tqdm\ntqdm.pandas()\ntest['rewrite_prompt'] = test.progress_apply(lambda s : predict(s, model, tokenizer, num_example=10), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:11:33.521898Z","iopub.execute_input":"2024-04-13T23:11:33.522337Z","iopub.status.idle":"2024-04-13T23:11:48.326172Z","shell.execute_reply.started":"2024-04-13T23:11:33.522304Z","shell.execute_reply":"2024-04-13T23:11:48.325143Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:14<00:00, 14.79s/it]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"# import swifter\n# tic = time.time()\n# test['rewrite_prompt'] = test.swifter.apply(lambda s : predict(s, model, tokenizer), axis=1)\n# toc = time.time()\n# print((toc-tic)/60)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:11:51.474345Z","iopub.execute_input":"2024-04-13T23:11:51.474761Z","iopub.status.idle":"2024-04-13T23:11:51.479622Z","shell.execute_reply.started":"2024-04-13T23:11:51.474730Z","shell.execute_reply":"2024-04-13T23:11:51.478496Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# preview\nif MODE == Mode.DEBUG:\n    display(test.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:11:51.914505Z","iopub.execute_input":"2024-04-13T23:11:51.914921Z","iopub.status.idle":"2024-04-13T23:11:51.919925Z","shell.execute_reply.started":"2024-04-13T23:11:51.914892Z","shell.execute_reply":"2024-04-13T23:11:51.918704Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# calculate score\n# load sentence-t5 base for embedding\n# todo\n# define sharped cosine simularity\n# todo\n# apply","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:11:52.706298Z","iopub.execute_input":"2024-04-13T23:11:52.707188Z","iopub.status.idle":"2024-04-13T23:11:52.711733Z","shell.execute_reply.started":"2024-04-13T23:11:52.707147Z","shell.execute_reply":"2024-04-13T23:11:52.710621Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# save the result as csv file \nif MODE == Mode.SUBMISSION:\n    test[['id', 'rewrite_prompt']].to_csv('submission.csv', header=True, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:11:53.325491Z","iopub.execute_input":"2024-04-13T23:11:53.326028Z","iopub.status.idle":"2024-04-13T23:11:53.339831Z","shell.execute_reply.started":"2024-04-13T23:11:53.325983Z","shell.execute_reply":"2024-04-13T23:11:53.338604Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"# Verification","metadata":{}},{"cell_type":"code","source":"if MODE == Mode.SUBMISSION:\n    display(pd.read_csv(\"/kaggle/working/submission.csv\").head())","metadata":{"execution":{"iopub.status.busy":"2024-04-13T23:11:55.022635Z","iopub.execute_input":"2024-04-13T23:11:55.023368Z","iopub.status.idle":"2024-04-13T23:11:55.036791Z","shell.execute_reply.started":"2024-04-13T23:11:55.023334Z","shell.execute_reply":"2024-04-13T23:11:55.035424Z"},"trusted":true},"execution_count":77,"outputs":[{"output_type":"display_data","data":{"text/plain":"   id                                     rewrite_prompt\n0  -1  Convert this into a sea shanty: \"\"\"The competi...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>rewrite_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>Convert this into a sea shanty: \"\"\"The competi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}