{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading llm-prompt-recovery.zip to d:\\PYTHON\\Vscode\\Kaggle\\LLMpromptrecovery\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/1.45k [00:00<?, ?B/s]\n",
      "100%|██████████| 1.45k/1.45k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c llm-prompt-recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data unzipped successfully\n"
     ]
    }
   ],
   "source": [
    "# Unzip the data \n",
    "try:\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile('llm-prompt-recovery.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "\n",
    "    print(\"Data unzipped successfully\")\n",
    "except Exception as e:\n",
    "    print(\"Error unzipping data\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma1000_w7b.csv.zip unzipped successfully\n",
      "nbroad-v1.csv.zip unzipped successfully\n",
      "nbroad-v2.csv.zip unzipped successfully\n",
      "prompts_0_500_wiki_first_para_3000.csv.zip unzipped successfully\n"
     ]
    }
   ],
   "source": [
    "## process all the zip file and extract \n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "dataset_path = os.path.join(current_dir, 'llmdataset')\n",
    "files = os.listdir(dataset_path)\n",
    "for file in files:\n",
    "    try:\n",
    "        with ZipFile(os.path.join(dataset_path, file), 'r') as zip_ref:\n",
    "            zip_ref.extractall('llmdataset_extracted/')\n",
    "        print(f\"{file} unzipped successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error unzipping {file}\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma1000_w7b.csv dataframe created successfully\n",
      "nbroad-v1.csv dataframe created successfully\n",
      "nbroad-v2.csv dataframe created successfully\n",
      "prompts_0_500_wiki_first_para_3000.csv dataframe created successfully\n"
     ]
    }
   ],
   "source": [
    "# create the dataframes \n",
    "data_frames = {}\n",
    "import pandas as pd\n",
    "for files in os.listdir('llmdataset_extracted/'):\n",
    "    if files.endswith('.csv'):\n",
    "        data_frames[files.split(\".\")[0]] = pd.read_csv(f'llmdataset_extracted/{files}')\n",
    "        print(f\"{files} dataframe created successfully\")\n",
    "    else:\n",
    "        print(f\"{files} is not a csv file\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of gemma1000_w7b dataframe: Index(['original_text', 'rewrite_prompt', 'rewritten_text'], dtype='object')\n",
      "Columns of nbroad-v1 dataframe: Index(['original_text', 'rewrite_prompt', 'rewritten_text'], dtype='object')\n",
      "Columns of nbroad-v2 dataframe: Index(['original_text', 'rewrite_prompt', 'rewritten_text'], dtype='object')\n",
      "Columns of prompts_0_500_wiki_first_para_3000 dataframe: Index(['original_text', 'rewrite_prompt', 'rewritten_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# streamline the dataframes\n",
    "# Iterate over the dictionary keys\n",
    "for key in data_frames.keys():\n",
    "    # Select columns based on the dataframe key\n",
    "    if key != list(data_frames.keys())[0]:\n",
    "        selected_columns = [\"original_text\", \"rewrite_prompt\", \"rewritten_text\"]\n",
    "    else:\n",
    "        selected_columns = [\"original_text\", \"rewrite_prompt\", \"gemma_7b_rewritten_text_temp0\"]\n",
    "        \n",
    "    # Select and rename columns\n",
    "    data_frames[key] = data_frames[key][selected_columns].rename(columns={\"gemma_7b_rewritten_text_temp0\": \"rewritten_text\"})\n",
    "    \n",
    "    # Print columns of the modified dataframe\n",
    "    print(f\"Columns of {key} dataframe: {data_frames[key].columns}\")\n",
    "\n",
    "# concatenate the dataframes\n",
    "# Concatenate the dataframes\n",
    "def concate_Dataframe(dataframe1,dataframe2):\n",
    "    return pd.concat([dataframe1, dataframe2], axis=0)\n",
    "df1 = concate_Dataframe(data_frames[list(data_frames.keys())[0]], data_frames[list(data_frames.keys())[1]])\n",
    "df2 = concate_Dataframe(data_frames[list(data_frames.keys())[2]], data_frames[list(data_frames.keys())[3]])\n",
    "df = concate_Dataframe(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the concatenated dataframe: (8566, 3)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the dataframe\n",
    "print(f\"Shape of the concatenated dataframe: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type1方法，过长\n",
    "```python\n",
    "\n",
    "# # # check the columns name of the dataframes\n",
    "# # for key in data_frames.keys():\n",
    "# #     print(f\"Columns of {key} dataframe: {data_frames[key].columns}\")\n",
    "# keys = list(data_frames.keys())\n",
    "\n",
    "# for key in list(data_frames.keys()):\n",
    "#     if key != list(data_frames.keys())[0]:\n",
    "#         data_frames[key] = data_frames[key][[\"original_text\",\"rewrite_prompt\",\"rewritten_text\"]]\n",
    "#     else: \n",
    "#         data_frames[key] = data_frames[key][[\"original_text\",\"rewrite_prompt\",\"gemma_7b_rewritten_text_temp0\"]]\n",
    "\n",
    "\n",
    "# for key in data_frames.keys():\n",
    "#     print(f\"Columns of {key} dataframe: {data_frames[key].columns}\")\n",
    "#     if key == keys[0]:\n",
    "#         data_frames[key].rename(columns={\"gemma_7b_rewritten_text_temp0\":\"rewritten_text\"}, inplace=True)\n",
    "\n",
    "# # check the columns name of the dataframes\n",
    "# for key in data_frames.keys():\n",
    "#     print(f\"Columns of {key} dataframe: {data_frames[key].columns}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we sample 3000 data as training data, 1000 as validation data and 1000 as test data\n",
    "train_data = df.sample(3000).reset_index(drop=True)\n",
    "validation_data = df.sample(1000).reset_index(drop=True)\n",
    "test_data = df.sample(1000).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data: (3000, 3)\n",
      "Shape of the validation data: (1000, 3)\n",
      "Shape of the test data: (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of the data\n",
    "print(f\"Shape of the training data: {train_data.shape}\")\n",
    "print(f\"Shape of the validation data: {validation_data.shape}\")\n",
    "print(f\"Shape of the test data: {test_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare the promopt engeneering\n",
    "template = \"\"\"Instruction:\\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\\n\\nOriginal Text:\\n{original_text}\\n\\nRewriten Text:\\n{rewritten_text}\\n\\nResponse:\\n{rewrite_prompt}\"\"\"\n",
    "\n",
    "# Apply the template to the training data\n",
    "train_data[\"prompt\"] = train_data.apply(lambda x: template.format(original_text=x[\"original_text\"], rewritten_text=x[\"rewritten_text\"], rewrite_prompt=x[\"rewrite_prompt\"]), axis=1)\n",
    "validation_data[\"prompt\"] = validation_data.apply(lambda x: template.format(original_text=x[\"original_text\"], rewritten_text=x[\"rewritten_text\"], rewrite_prompt=x[\"rewrite_prompt\"]), axis=1)\n",
    "test_data[\"prompt\"] = test_data.apply(lambda x: template.format(original_text=x[\"original_text\"], rewritten_text=x[\"rewritten_text\"], rewrite_prompt=x[\"rewrite_prompt\"]), axis=1)\n",
    "\n",
    "data = train_data.prompt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "Below, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n",
      "\n",
      "Original Text:\n",
      "The 1945 Chattanooga Moccasins football team was an American football team that represented the University of Chattanooga (now known as the University of Tennessee at Chattanooga) as an independent during the 1945 college football season. In its 15th year under head coach Scrappy Moore, the team compiled a 5–3 record.\n",
      "\n",
      "Rewriten Text:\n",
      "## Apprentice Magician's Guide: The 1945 Chattanooga Moccasins\n",
      "\n",
      "**Gather ye curious and eager to learn, young apprentice,** and let's delve into the tale of the 1945 Chattanooga Moccasins, a team etched in the annals of magic and myth.\n",
      "\n",
      "**The Moccasins danced upon the gridiron, a force of fiery power and nimble grace.** They represented the University of Chattanooga, once a beacon of learning disguised in the shell of a ferocious football team. Led by the enigmatic head coach Scrappy Moore, the Moccasins were enigma wrapped in a mystery of potent throws and\n",
      "\n",
      "Response:\n",
      "Convert it into a guide for apprentice magicians.\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Markdown'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mMarkdown\u001b[39;00m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcolorize_text\u001b[39m(text):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Markdown'"
     ]
    }
   ],
   "source": [
    "# import Markdown \n",
    "# from IPython.display import display\n",
    "# def colorize_text(text):\n",
    "#     for word, color in zip([\"Instruction\", \"Original Text\", \"Rewriten Text\", \"Response\"],\n",
    "#                            [\"red\", \"yellow\", \"blue\", \"green\"]):\n",
    "#         text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "#     return text\n",
    "\n",
    "# # Take a random sample\n",
    "# sample = data[10]\n",
    "\n",
    "# # Give colors to Instruction, Response and Category\n",
    "# sample = colorize_text(sample)\n",
    "\n",
    "# # Show sample in markdown\n",
    "# display(Markdown(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\PYTHON\\cacheTransformers\n"
     ]
    }
   ],
   "source": [
    "# check all the env\n",
    "print(os.environ[\"HF_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\PYTHON\\cacheTransformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PYTHON\\ENV\\labsession\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\PYTHON\\cacheTransformers\\hub\\models--google--gemma-7b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 4/4 [09:22<00:00, 140.55s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# check the hugging face environment\n",
    "print(os.environ['HF_HOME'])\n",
    "access_token = \"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\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\",token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\",token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaConfig {\n",
      "  \"_name_or_path\": \"google/gemma-7b\",\n",
      "  \"architectures\": [\n",
      "    \"GemmaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 24576,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the summary of the model\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (down_proj): Linear(in_features=24576, out_features=3072, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 8537680896\n"
     ]
    }
   ],
   "source": [
    "# totoal number of parameters\n",
    "print(f\"Total number of parameters: {model.num_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune the model \n",
    "test1= tokenizer(data[0:2], padding=True, truncation=True, return_tensors=\"pt\", return_attention_mask=True, return_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      2,  37854, 235292,    108,\n",
       "          33501, 235269,    573,   4103,  16221,   4820, 235376,  14732,    919,\n",
       "           1125,  86906, 235283, 110235, 235283, 120772,   1280,   4103,    987,\n",
       "          25513,   4820, 235376,    731,    573,   4103, 204604, 235248, 235324,\n",
       "         235268, 235290,    500, 235376,    629,  18622,    675,    476,   3383,\n",
       "          18335, 235283,  35722, 235265,   3883,   6911,    603,    577,  13237,\n",
       "          27205,    573,  10216,   1865,    573,   4103,  16221,   4820, 235376,\n",
       "            578,   4103,    987,  25513,   4820,  10738,    578,   3418,    577,\n",
       "          12653,    573,   3724,  18335,    689,  14239,    674,    729,   5476,\n",
       "           2764,    577,    573,    629,  18622,    577,  60358, 235283,  10577,\n",
       "         235283, 100858,    573,   2793,    575,    736,   1703, 235265,    109,\n",
       "          16221,   4820, 235292,    108,    651, 235248, 235274, 235315, 235310,\n",
       "         235308, 136237,    595,  42866,   1137,   9715,   2970,    729,    671,\n",
       "           3725,   9715,   2970,    674,  12754,    573,   2895,    576, 136237,\n",
       "            591,   1032,   3836,    685,    573,   2895,    576,  26447,    696,\n",
       "         136237, 235275,    685,    671,   9560,   2290,    573, 235248, 235274,\n",
       "         235315, 235310, 235308,   9222,   9715,   3891, 235265,    878,   1277,\n",
       "         235248, 235274, 235308,    489,   1162,   1362,   2206,   9266,  94078,\n",
       "           6603,  19413, 235269,    573,   2970,  33544,    476, 235248, 235308,\n",
       "         235389, 235304,   3661, 235265,    109,    987,  12092,    479,   4820,\n",
       "         235292,    108,   1620, 154599, 179810, 235303, 235256,   9345, 235292,\n",
       "            714, 235248, 235274, 235315, 235310, 235308, 136237,    595,  42866,\n",
       "           1137,    109,    688,  94263,   9929,  17978,    578,  26135,    577,\n",
       "           3918, 235269,   3486,  90852, 218215,    578,   2142, 235303, 235256,\n",
       "         144849,   1280,    573,  20473,    576,    573, 235248, 235274, 235315,\n",
       "         235310, 235308, 136237,    595,  42866,   1137, 235269,    476,   2970,\n",
       "         105866,    575,    573, 169816,    576,  11908,    578,  25184, 235265,\n",
       "            109,    688,    651,    595,  42866,   1137,  77888,   3054,    573,\n",
       "           8704,    543,   1695, 235269,    476,   5097,    576,  71684,   2384,\n",
       "            578, 177002,  15434, 116742,   2365,  12754,    573,   2895,    576,\n",
       "         136237, 235269,   3631,    476,  87447,    576,   6044, 105513,    575,\n",
       "            573,  14768,    576,    476, 135203,   9715,   2970, 235265,  33013,\n",
       "            731,    573, 191040,   2206,   9266,  94078,   6603,  19413, 235269,\n",
       "            573,    595,  42866,   1137,   1049, 148290,  26873,    575,    476,\n",
       "          25418,    576,  53518,   8129,    578,    109,   3943, 235292,    108,\n",
       "          29039,    665,   1280,    476,   5608,    604,  90852, 216364, 235265],\n",
       "        [     2,  37854, 235292,    108,  33501, 235269,    573,   4103,  16221,\n",
       "           4820, 235376,  14732,    919,   1125,  86906, 235283, 110235, 235283,\n",
       "         120772,   1280,   4103,    987,  25513,   4820, 235376,    731,    573,\n",
       "           4103, 204604, 235248, 235324, 235268, 235290,    500, 235376,    629,\n",
       "          18622,    675,    476,   3383,  18335, 235283,  35722, 235265,   3883,\n",
       "           6911,    603,    577,  13237,  27205,    573,  10216,   1865,    573,\n",
       "           4103,  16221,   4820, 235376,    578,   4103,    987,  25513,   4820,\n",
       "          10738,    578,   3418,    577,  12653,    573,   3724,  18335,    689,\n",
       "          14239,    674,    729,   5476,   2764,    577,    573,    629,  18622,\n",
       "            577,  60358, 235283,  10577, 235283, 100858,    573,   2793,    575,\n",
       "            736,   1703, 235265,    109,  16221,   4820, 235292,    108, 235277,\n",
       "         199337,   1066, 235376, 235256,  33542, 235269,    476,  19634,   5774,\n",
       "            576,    671,  37888, 235269,    603,    974,    576,    573,  22013,\n",
       "          10162,    553,    480,   9437, 235265,   1165,    729,   1644,    696,\n",
       "            573,   1580,    576,    573, 235248, 235321,    489,   7861,    575,\n",
       "            573,  12108,  56734, 163237,    578,    729,   1942,    575,    573,\n",
       "         235248, 235274, 235315,    489,   7861,    575,    573,   9552,    576,\n",
       "           3984,   3423,    576,    573,   9576,    576,  10162,    553,    480,\n",
       "           9437, 235265,    109,    987,  12092,    479,   4820, 235292,    108,\n",
       "           1620,   8542,   9016,  41265, 235292,  80729,    576,  10162,    553,\n",
       "            480,   9437,    109,    688,   8666,  13653, 218215,    109,   1734,\n",
       "         235303,    478,  12826,    577,   4638,    671,   4601,   1105,    573,\n",
       "          22013,    576,  10162,    553,    480,   9437, 235292,    109, 235277,\n",
       "         199337,   1066, 235303, 235256,  33542, 235269,    476,  19634,   5774,\n",
       "            576,    671,  37888, 235269,    603,    476,  10159,  20359,  53645,\n",
       "            575,    573,  25949,    576,    573,   9576,    576,  10162,    553,\n",
       "            480,   9437, 235265,  67674,  57964,    696,    573,   1580,    576,\n",
       "            573, 235248, 235321,    489,   7861,   2290,    573,  12108,  56734,\n",
       "         163237, 235269,    736,   4937,  70752,    729,  13437,    575,    573,\n",
       "         235248, 235274, 235315,    489,   7861,   2819,    573,   9552,    576,\n",
       "           3984,   3423, 235265,    109,   1596,  13457,  10159,  12786,    573,\n",
       "          25263,    578,   7268,    576,    573,  10162,   1838,   1461, 235265,\n",
       "           1165,  19179,    685,    476,  10159,    576,  20065, 235269,  17564,\n",
       "         235269,    578,  53369, 235265,    109,   1734, 235303,    478,  13221,\n",
       "            577,  54785,    573,   7277,  10831,  25949,    576,  10162,    553,\n",
       "            480,   9437,    578,   4564,    109,   3943, 235292,    108,  95084,\n",
       "            736,    685,    476,   5075,   4601,  21244,    577,   7138, 235265]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([2, 333])\n",
      "attention_mask: torch.Size([2, 333])\n"
     ]
    }
   ],
   "source": [
    "keys_1 = list(test1.keys())\n",
    "for key in keys_1:\n",
    "    print(f\"{key}: {test1[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PYTHON\\ENV\\labsession\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:561: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# have the first try \n",
    "output = model.generate(**test1,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><bos>Instruction:\n",
      "Below, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n",
      "\n",
      "Original Text:\n",
      "The 1945 Chattanooga Moccasins football team was an American football team that represented the University of Chattanooga (now known as the University of Tennessee at Chattanooga) as an independent during the 1945 college football season. In its 15th year under head coach Scrappy Moore, the team compiled a 5–3 record.\n",
      "\n",
      "Rewriten Text:\n",
      "## Apprentice Magician's Guide: The 1945 Chattanooga Moccasins\n",
      "\n",
      "**Gather ye curious and eager to learn, young apprentice,** and let's delve into the tale of the 1945 Chattanooga Moccasins, a team etched in the annals of magic and myth.\n",
      "\n",
      "**The Moccasins danced upon the gridiron, a force of fiery power and nimble grace.** They represented the University of Chattanooga, once a beacon of learning disguised in the shell of a ferocious football team. Led by the enigmatic head coach Scrappy Moore, the Moccasins were enigma wrapped in a mystery of potent throws and\n",
      "\n",
      "Response:\n",
      "Convert it into a guide for apprentice magicians.<eos>\n"
     ]
    }
   ],
   "source": [
    "# check the output\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "Below, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n",
      "\n",
      "Original Text:\n",
      "The 1945 Chattanooga Moccasins football team was an American football team that represented the University of Chattanooga (now known as the University of Tennessee at Chattanooga) as an independent during the 1945 college football season. In its 15th year under head coach Scrappy Moore, the team compiled a 5–3 record.\n",
      "\n",
      "Rewriten Text:\n",
      "## Apprentice Magician's Guide: The 1945 Chattanooga Moccasins\n",
      "\n",
      "**Gather ye curious and eager to learn, young apprentice,** and let's delve into the tale of the 1945 Chattanooga Moccasins, a team etched in the annals of magic and myth.\n",
      "\n",
      "**The Moccasins danced upon the gridiron, a force of fiery power and nimble grace.** They represented the University of Chattanooga, once a beacon of learning disguised in the shell of a ferocious football team. Led by the enigmatic head coach Scrappy Moore, the Moccasins were enigma wrapped in a mystery of potent throws and\n",
      "\n",
      "Response:\n",
      "Convert it into a guide for apprentice magicians.\n"
     ]
    }
   ],
   "source": [
    "#compare with the original text\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing layer will take in batches of strings, and return outputs in a (x, y, sample_weight) format, where the y label is the next token id in the x sequence.\n",
    "\n",
    "From the code below, we can see that, after the preprocessor, the data shape is (num_samples, sequence_length)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labsession",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
